# IMAGE-CAPTIONING-USING-XCEPTION-LSTM
The purpose of this technology is to describe the content of images, with potential applications including aiding visually impaired individuals and supporting self-driving vehicles. Describing an image accurately is a complex task, often requiring appropriate sentence structure. However, this process becomes simplified and more straightforward through the application of deep learning techniques. We will employ Convolutional Neural Networks (CNN), specifically a pre-trained Xception model, and Long Short-Term Memory (LSTM) algorithms to achieve this goal. Convolutional neural networks are deep learning algorithms designed to extract features from input images. These extracted features are then passed to the subsequent layer, which feeds into LSTM. LSTM, in turn, utilizes a sequence processor to arrange sentences in a coherent order and construct the final caption for the input image. The described CNN model incorporates depth-wise separable convolutions, and we will employ the Flickr_8k dataset for training and testing purposes. 
The outcome of this system is that when an image is provided as input, it generates a descriptive caption for the image. Additionally, the generated caption can be converted to audio and read out to the user, enhancing its accessibility.

FOR FULL MODEL ACCESS GO TO:
https://drive.google.com/drive/folders/1xsfzYUBGm0yr7PbO336yCsWQx3xsNkRX?usp=sharing

SOFTWARE USED: VS CODE AND PYCHAM PRO
